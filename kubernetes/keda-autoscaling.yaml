# KEDA Autoscaling Configuration for Aura AI Platform
# Advanced autoscaling based on Prometheus metrics and HTTP requests

# KEDA ScaledObject for Visual Analysis Service
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: visual-analysis-scaler
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: visual-analysis
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    name: visual-analysis-deployment
  pollingInterval: 30  # Check metrics every 30 seconds
  cooldownPeriod: 300  # Wait 5 minutes before scaling down
  idleReplicaCount: 1  # Keep 1 replica when idle
  minReplicaCount: 2   # Minimum replicas
  maxReplicaCount: 15  # Maximum replicas
  
  triggers:
  # Scale based on HTTP requests per second
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: http_requests_per_second
      threshold: '50'  # Scale up when RPS > 50
      query: |
        sum(rate(nginx_http_requests_total{service="visual-analysis"}[2m]))
  
  # Scale based on average response time
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: avg_response_time_ms
      threshold: '2000'  # Scale up when response time > 2s
      query: |
        avg(histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service="visual-analysis"}[5m]))) * 1000
  
  # Scale based on GPU utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: gpu_utilization_percent
      threshold: '80'  # Scale up when GPU utilization > 80%
      query: |
        avg(nvidia_gpu_utilization{service="visual-analysis"})
  
  # Scale based on queue depth (pending requests)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: queue_depth
      threshold: '10'  # Scale up when queue depth > 10
      query: |
        sum(nginx_http_connections_waiting{service="visual-analysis"})

  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 10
            periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 50
            periodSeconds: 60
          - type: Pods
            value: 3
            periodSeconds: 60
          selectPolicy: Max

---
# KEDA ScaledObject for Outfit Recommendation Service
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: outfit-recommendation-scaler
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: outfit-recommendation
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    name: outfit-recommendation-deployment
  pollingInterval: 30
  cooldownPeriod: 300
  idleReplicaCount: 1
  minReplicaCount: 2
  maxReplicaCount: 12
  
  triggers:
  # Scale based on HTTP requests per second
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: http_requests_per_second
      threshold: '30'  # Lower threshold due to higher complexity
      query: |
        sum(rate(nginx_http_requests_total{service="outfit-recommendation"}[2m]))
  
  # Scale based on CPU utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: cpu_utilization_percent
      threshold: '70'
      query: |
        avg(rate(container_cpu_usage_seconds_total{pod=~"outfit-recommendation-.*"}[5m])) * 100
  
  # Scale based on memory utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: memory_utilization_percent
      threshold: '75'
      query: |
        avg(container_memory_usage_bytes{pod=~"outfit-recommendation-.*"} / container_spec_memory_limit_bytes{pod=~"outfit-recommendation-.*"}) * 100
  
  # Scale based on model inference time
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: inference_time_ms
      threshold: '5000'  # Scale up when inference time > 5s
      query: |
        avg(histogram_quantile(0.95, rate(model_inference_duration_seconds_bucket{service="outfit-recommendation"}[5m]))) * 1000

  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 15
            periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 75
            periodSeconds: 60
          - type: Pods
            value: 2
            periodSeconds: 60
          selectPolicy: Max

---
# KEDA ScaledObject for Conversational AI Service
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: conversational-ai-scaler
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: conversational-ai
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    name: conversational-ai-deployment
  pollingInterval: 45  # Longer polling for LLM service
  cooldownPeriod: 600  # Longer cooldown due to high startup cost
  idleReplicaCount: 1
  minReplicaCount: 1
  maxReplicaCount: 6   # Limited scaling due to memory requirements
  
  triggers:
  # Scale based on active WebSocket connections
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: active_websocket_connections
      threshold: '10'  # Scale up when > 10 active connections per pod
      query: |
        sum(websocket_connections_active{service="conversational-ai"}) / count(up{service="conversational-ai"})
  
  # Scale based on message queue depth
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: message_queue_depth
      threshold: '5'   # Scale up when > 5 messages queued
      query: |
        sum(chat_messages_queued{service="conversational-ai"})
  
  # Scale based on LLM processing time
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: llm_processing_time_ms
      threshold: '10000'  # Scale up when processing time > 10s
      query: |
        avg(histogram_quantile(0.95, rate(llm_inference_duration_seconds_bucket{service="conversational-ai"}[5m]))) * 1000
  
  # Scale based on GPU memory utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: gpu_memory_utilization_percent
      threshold: '85'  # Scale up when GPU memory > 85%
      query: |
        avg(nvidia_gpu_memory_used_bytes{service="conversational-ai"} / nvidia_gpu_memory_total_bytes{service="conversational-ai"}) * 100

  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 600  # Very conservative scaling down
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
        scaleUp:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 100
            periodSeconds: 120
          - type: Pods
            value: 1
            periodSeconds: 120
          selectPolicy: Max

---
# KEDA ScaledObject for Triton Inference Server
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: triton-scaler
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: triton-inference-server
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    name: triton-inference-server-deployment
  pollingInterval: 15  # Faster polling for inference server
  cooldownPeriod: 180  # Shorter cooldown for responsive scaling
  idleReplicaCount: 2
  minReplicaCount: 2
  maxReplicaCount: 10
  
  triggers:
  # Scale based on inference requests per second
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: inference_rps
      threshold: '100'  # Scale up when inference RPS > 100
      query: |
        sum(rate(triton_inference_request_success_total[2m]))
  
  # Scale based on model queue time
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: model_queue_time_ms
      threshold: '100'  # Scale up when queue time > 100ms
      query: |
        avg(triton_inference_queue_duration_us) / 1000
  
  # Scale based on batch utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: batch_utilization_percent
      threshold: '80'   # Scale up when batch utilization > 80%
      query: |
        avg(triton_inference_batch_size / triton_inference_max_batch_size) * 100
  
  # Scale based on GPU utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: triton_gpu_utilization
      threshold: '75'
      query: |
        avg(nvidia_gpu_utilization{service="triton"})

  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 30
        scaleUp:
          stabilizationWindowSeconds: 30
          policies:
          - type: Percent
            value: 100
            periodSeconds: 30
          - type: Pods
            value: 3
            periodSeconds: 30
          selectPolicy: Max

---
# KEDA ScaledObject for Nginx Gateway
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: nginx-gateway-scaler
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: nginx-gateway
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    name: nginx-gateway-deployment
  pollingInterval: 15
  cooldownPeriod: 120  # Quick scaling for gateway
  idleReplicaCount: 2
  minReplicaCount: 3
  maxReplicaCount: 20
  
  triggers:
  # Scale based on total HTTP requests
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: total_http_rps
      threshold: '500'  # Scale up when total RPS > 500
      query: |
        sum(rate(nginx_http_requests_total[2m]))
  
  # Scale based on connection queue
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: nginx_connections_waiting
      threshold: '50'   # Scale up when > 50 connections waiting
      query: |
        sum(nginx_connections_waiting)
  
  # Scale based on response time
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: nginx_response_time_ms
      threshold: '1000'  # Scale up when response time > 1s
      query: |
        avg(nginx_http_request_duration_seconds_bucket{le="1.0"}) * 1000
  
  # Scale based on CPU utilization
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-service.monitoring.svc.cluster.local:9090
      metricName: nginx_cpu_utilization
      threshold: '70'
      query: |
        avg(rate(container_cpu_usage_seconds_total{pod=~"nginx-gateway-.*"}[2m])) * 100

  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 120
          policies:
          - type: Percent
            value: 20
            periodSeconds: 30
        scaleUp:
          stabilizationWindowSeconds: 30
          policies:
          - type: Percent
            value: 100
            periodSeconds: 15
          - type: Pods
            value: 5
            periodSeconds: 30
          selectPolicy: Max

---
# KEDA TriggerAuthentication for Prometheus
apiVersion: keda.sh/v1alpha1
kind: TriggerAuthentication
metadata:
  name: prometheus-trigger-auth
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: aura-ai-platform
    app.kubernetes.io/component: keda-auth
spec:
  secretTargetRef:
  - parameter: username
    name: prometheus-auth
    key: username
  - parameter: password
    name: prometheus-auth
    key: password

---
# Prometheus Authentication Secret (if needed)
apiVersion: v1
kind: Secret
metadata:
  name: prometheus-auth
  namespace: aura-ai
  labels:
    app.kubernetes.io/name: aura-ai-platform
    app.kubernetes.io/component: prometheus-auth
type: Opaque
data:
  username: cHJvbWV0aGV1cw==  # prometheus
  password: cHJvbWV0aGV1c19wYXNzd29yZA==  # prometheus_password
