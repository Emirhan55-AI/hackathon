"""
RAG Service Resource Manager - Bellek SÄ±zÄ±ntÄ±sÄ± KorumasÄ±
Bu modÃ¼l, RAGService'in kaynaklarÄ±nÄ±n gÃ¼venli bir ÅŸekilde yÃ¶netilmesini saÄŸlar.
BÃ¼yÃ¼k modeller ve GPU belleÄŸi iÃ§in bellek sÄ±zÄ±ntÄ±sÄ± korumasÄ± sunar.
"""

import weakref
import logging
import gc
import time
from typing import Optional, TYPE_CHECKING
from contextlib import contextmanager

if TYPE_CHECKING:
    from src.rag_service import RAGService

# Logger setup
logger = logging.getLogger(__name__)

# Global service tracking for debugging
_service_refs = weakref.WeakSet()


class RAGServiceResource:
    """
    RAGService iÃ§in context manager ve kaynak yÃ¶neticisi.
    Bellek sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek iÃ§in bÃ¼yÃ¼k modellerin referanslarÄ±nÄ± yÃ¶netir.
    """
    
    def __init__(self, service: 'RAGService'):
        """
        RAGServiceResource'Ä± baÅŸlatÄ±r
        
        Args:
            service: YÃ¶netilecek RAGService instance'Ä±
        """
        self.service = service
        self._service_id = id(service) if service else None
        self._entered = False
        self._cleanup_called = False
        
        if service:
            logger.debug(f"ðŸ”§ RAGServiceResource oluÅŸturuldu: {self._service_id}")
            # Servisi takip et
            track_service(service)
        else:
            logger.warning("âš ï¸ RAGServiceResource None service ile oluÅŸturuldu")
    
    def __enter__(self):
        """
        Context manager giriÅŸ noktasÄ±
        
        Returns:
            RAGService: YÃ¶netilen service instance'Ä±
        """
        self._entered = True
        
        if self.service:
            logger.debug(f"ðŸ”„ RAGServiceResource girildi: {self._service_id}")
        else:
            logger.warning("âš ï¸ RAGServiceResource None service ile girildi")
        
        return self.service
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Context manager Ã§Ä±kÄ±ÅŸ noktasÄ± - Kaynak temizleme
        
        Args:
            exc_type: Exception tipi
            exc_val: Exception deÄŸeri  
            exc_tb: Exception traceback
        """
        logger.debug(f"ðŸ”„ RAGServiceResource Ã§Ä±kÄ±lÄ±yor: {self._service_id}")
        
        try:
            # Service cleanup Ã§aÄŸÄ±r
            if self.service and hasattr(self.service, 'cleanup'):
                try:
                    logger.debug(f"ðŸ§¹ RAGService cleanup Ã§aÄŸrÄ±lÄ±yor: {self._service_id}")
                    self.service.cleanup()
                    self._cleanup_called = True
                except Exception as e:
                    logger.error(f"âŒ RAGService cleanup hatasÄ±: {str(e)}")
            
            # PyTorch GPU bellek temizliÄŸi
            try:
                import torch
                if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                    logger.debug("ðŸ–¥ï¸ GPU bellek cache temizleniyor...")
                    torch.cuda.empty_cache()
                    
                    # GPU bellek istatistikleri
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated()
                        cached = torch.cuda.memory_reserved()
                        logger.debug(f"ðŸ“Š GPU Bellek - Allocated: {allocated/1024**2:.1f}MB, Cached: {cached/1024**2:.1f}MB")
                        
            except ImportError:
                logger.debug("ðŸ“ PyTorch bulunamadÄ±, GPU temizleme atlandÄ±")
            except Exception as e:
                logger.warning(f"âš ï¸ GPU bellek temizleme hatasÄ±: {str(e)}")
            
            # Service referansÄ±nÄ± temizle
            if self.service:
                logger.debug(f"ðŸ”— Service referansÄ± temizleniyor: {self._service_id}")
                self.service = None
            
            # Agresif garbage collection (sadece gerektiÄŸinde)
            if self._cleanup_called:
                logger.debug("ðŸ—‘ï¸ Garbage collection tetikleniyor...")
                collected = gc.collect()
                if collected > 0:
                    logger.debug(f"ðŸ—‘ï¸ {collected} nesne garbage collection ile temizlendi")
                    
        except Exception as e:
            logger.error(f"âŒ RAGServiceResource Ã§Ä±kÄ±ÅŸ hatasÄ±: {str(e)}")
        
        finally:
            self._entered = False
            logger.debug(f"âœ… RAGServiceResource Ã§Ä±kÄ±ldÄ±: {self._service_id}")
            
        # Exception'Ä± yeniden fÄ±rlatma (False dÃ¶ndÃ¼r)
        return False
    
    def force_cleanup(self):
        """
        Zorunlu temizleme iÅŸlemi (emergency use)
        """
        if not self._cleanup_called and self.service:
            logger.warning(f"âš ï¸ Force cleanup Ã§aÄŸrÄ±ldÄ±: {self._service_id}")
            self.__exit__(None, None, None)
    
    def is_valid(self) -> bool:
        """
        Resource'un geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder
        
        Returns:
            bool: Resource geÃ§erliyse True
        """
        return self.service is not None and not self._cleanup_called
    
    def get_memory_stats(self) -> dict:
        """
        Bellek istatistiklerini dÃ¶ndÃ¼rÃ¼r
        
        Returns:
            dict: Bellek durumu bilgileri
        """
        stats = {
            "service_valid": self.is_valid(),
            "cleanup_called": self._cleanup_called,
            "service_id": self._service_id,
            "tracked_services": get_tracked_service_count()
        }
        
        # PyTorch GPU stats
        try:
            import torch
            if torch.cuda.is_available():
                stats.update({
                    "gpu_allocated_mb": torch.cuda.memory_allocated() / 1024**2,
                    "gpu_cached_mb": torch.cuda.memory_reserved() / 1024**2,
                    "gpu_device_count": torch.cuda.device_count()
                })
        except ImportError:
            stats["gpu_info"] = "PyTorch not available"
        except Exception as e:
            stats["gpu_error"] = str(e)
        
        return stats
    
    def __del__(self):
        """
        Destructor - son gÃ¼venlik Ã¶nlemi
        """
        if self.service and not self._cleanup_called:
            logger.warning(f"âš ï¸ RAGServiceResource destructor cleanup: {self._service_id}")
            try:
                self.force_cleanup()
            except:
                pass


# Service tracking functions
def track_service(service: 'RAGService') -> None:
    """
    RAGService instance'Ä±nÄ± weak reference ile takip eder
    
    Args:
        service: Takip edilecek RAGService instance'Ä±
    """
    if service:
        _service_refs.add(service)
        service_id = id(service)
        logger.debug(f"ðŸ“Š Service tracked: {service_id} (Toplam: {len(_service_refs)})")


def get_tracked_service_count() -> int:
    """
    Takip edilen service sayÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r
    
    Returns:
        int: Aktif service sayÄ±sÄ±
    """
    return len(_service_refs)


def get_tracked_services_info() -> dict:
    """
    Takip edilen servislerin detaylÄ± bilgilerini dÃ¶ndÃ¼rÃ¼r
    
    Returns:
        dict: Service bilgileri
    """
    services_info = []
    
    for service in _service_refs:
        try:
            service_info = {
                "id": id(service),
                "type": type(service).__name__,
                "model_loaded": hasattr(service, 'model') and service.model is not None,
                "tokenizer_loaded": hasattr(service, 'tokenizer') and service.tokenizer is not None,
                "embedding_model_loaded": hasattr(service, 'embedding_model') and service.embedding_model is not None,
                "vector_store_loaded": hasattr(service, 'vector_store') and service.vector_store is not None,
            }
            services_info.append(service_info)
        except Exception as e:
            logger.warning(f"âš ï¸ Service info alma hatasÄ±: {str(e)}")
    
    return {
        "total_tracked": len(_service_refs),
        "services": services_info,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }


def clear_all_tracked_services() -> int:
    """
    TÃ¼m takip edilen servisleri temizler (debugging iÃ§in)
    
    Returns:
        int: Temizlenen service sayÄ±sÄ±
    """
    count = len(_service_refs)
    _service_refs.clear()
    logger.info(f"ðŸ§¹ {count} tracked service temizlendi")
    return count


@contextmanager
def rag_service_context(service: 'RAGService'):
    """
    RAGService iÃ§in context manager helper fonksiyonu
    
    Args:
        service: YÃ¶netilecek RAGService instance'Ä±
        
    Yields:
        RAGService: YÃ¶netilen service instance'Ä±
    """
    resource = RAGServiceResource(service)
    try:
        with resource as managed_service:
            yield managed_service
    finally:
        # Resource otomatik olarak temizlenecek
        pass


def force_cleanup_all_resources() -> dict:
    """
    Acil durum bellek temizleme fonksiyonu
    
    Returns:
        dict: Temizleme sonucu bilgileri
    """
    logger.warning("ðŸš¨ Force cleanup all resources Ã§aÄŸrÄ±ldÄ±")
    
    cleanup_stats = {
        "tracked_services_before": get_tracked_service_count(),
        "gpu_cleanup": False,
        "gc_collected": 0,
        "errors": []
    }
    
    try:
        # GPU bellek temizliÄŸi
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                cleanup_stats["gpu_cleanup"] = True
                logger.info("ðŸ–¥ï¸ GPU bellek force cleanup tamamlandÄ±")
        except Exception as e:
            cleanup_stats["errors"].append(f"GPU cleanup error: {str(e)}")
        
        # Aggressive garbage collection
        for _ in range(3):  # 3 defa gc.collect() Ã§aÄŸÄ±r
            collected = gc.collect()
            cleanup_stats["gc_collected"] += collected
        
        # Tracked services temizle
        cleared_count = clear_all_tracked_services()
        cleanup_stats["tracked_services_cleared"] = cleared_count
        
        logger.info(f"ðŸ§¹ Force cleanup tamamlandÄ±: {cleanup_stats}")
        
    except Exception as e:
        cleanup_stats["errors"].append(f"General cleanup error: {str(e)}")
        logger.error(f"âŒ Force cleanup hatasÄ±: {str(e)}")
    
    return cleanup_stats


# Health check fonksiyonu
def get_resource_health() -> dict:
    """
    Kaynak yÃ¶netimi saÄŸlÄ±k durumunu dÃ¶ndÃ¼rÃ¼r
    
    Returns:
        dict: SaÄŸlÄ±k durumu bilgileri
    """
    health_info = {
        "status": "healthy",
        "tracked_services": get_tracked_service_count(),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # GPU durumu
    try:
        import torch
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated()
            cached = torch.cuda.memory_reserved()
            max_memory = torch.cuda.get_device_properties(0).total_memory
            
            health_info["gpu"] = {
                "available": True,
                "allocated_mb": allocated / 1024**2,
                "cached_mb": cached / 1024**2,
                "total_mb": max_memory / 1024**2,
                "utilization_percent": (allocated / max_memory) * 100
            }
            
            # YÃ¼ksek bellek kullanÄ±mÄ± uyarÄ±sÄ±
            if (allocated / max_memory) > 0.8:
                health_info["status"] = "warning"
                health_info["warning"] = "High GPU memory utilization"
                
        else:
            health_info["gpu"] = {"available": False}
            
    except ImportError:
        health_info["gpu"] = {"available": False, "reason": "PyTorch not installed"}
    except Exception as e:
        health_info["gpu"] = {"available": False, "error": str(e)}
    
    return health_info
