# Aura Fashion Assistant - QLoRA Fine-Tuning Module

Bu modÃ¼l, Aura projesinin sohbet asistanÄ± iÃ§in Meta-Llama-3-8B-Instruct modelini QLoRA (Quantized LoRA) tekniÄŸi ile moda alanÄ±na Ã¶zel olarak ince ayar yapmak iÃ§in geliÅŸtirilmiÅŸtir.

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#Ã¶zellikler)
- [Kurulum](#kurulum)
- [Veri Seti HazÄ±rlÄ±ÄŸÄ±](#veri-seti-hazÄ±rlÄ±ÄŸÄ±)
- [KullanÄ±m](#kullanÄ±m)
- [KonfigÃ¼rasyon](#konfigÃ¼rasyon)
- [Sistem Gereksinimleri](#sistem-gereksinimleri)
- [Teknik Detaylar](#teknik-detaylar)
- [Troubleshooting](#troubleshooting)

## âœ¨ Ã–zellikler

- **QLoRA Fine-Tuning**: 4-bit quantization ile bellek verimli eÄŸitim
- **Fashion-Specific Personality**: Moda alanÄ±na Ã¶zel kiÅŸilik geliÅŸtirme
- **Memory Optimized**: Tek GPU'da (16GB+) Ã§alÄ±ÅŸacak ÅŸekilde optimize edildi
- **Production Ready**: FastAPI servisinde kullanÄ±ma hazÄ±r
- **Comprehensive Logging**: DetaylÄ± loglama ve monitoring desteÄŸi
- **Flexible Configuration**: FarklÄ± donanÄ±m iÃ§in optimize edilmiÅŸ konfigÃ¼rasyonlar

## ğŸš€ Kurulum

### 1. Gereksinimler

```bash
# Python 3.8+ gerekli
python --version

# CUDA destekli PyTorch kurulumu
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Proje baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± yÃ¼kle
cd services/conversational_ai
pip install -r requirements.txt
```

### 2. Model Ä°ndirme Ä°zinleri

Meta-Llama modelleri iÃ§in Hugging Face Hub'da giriÅŸ yapmanÄ±z gerekiyor:

```bash
# Hugging Face CLI ile giriÅŸ
huggingface-cli login

# Meta-Llama-3-8B-Instruct iÃ§in izin isteÄŸi:
# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```

## ğŸ“Š Veri Seti HazÄ±rlÄ±ÄŸÄ±

### 1. Women's Clothing Reviews Dataset

Kaggle'dan [Women's E-Commerce Clothing Reviews](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews) veri setini indirin:

```bash
# Veri seti klasÃ¶rÃ¼ne yerleÅŸtirin
mkdir -p data
# womens_clothing_reviews.csv dosyasÄ±nÄ± data/ klasÃ¶rÃ¼ne kopyalayÄ±n
```

### 2. Veri Seti FormatÄ±

CSV dosyasÄ±nda ÅŸu sÃ¼tunlar olmalÄ±:
- `Review Text`: MÃ¼ÅŸteri yorumlarÄ±
- `Title`: Yorum baÅŸlÄ±klarÄ± (opsiyonel)

### 3. Ã–zel Veri Seti

Kendi veri setinizi kullanmak iÃ§in aynÄ± format ile CSV oluÅŸturun:

```csv
Review Text,Title
"Bu elbise harika! Ã‡ok rahat ve ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor.","MuhteÅŸem Elbise"
"AyakkabÄ± tam beden geldi, kalitesi Ã§ok iyi.","Kaliteli AyakkabÄ±"
```

## ğŸ¯ KullanÄ±m

### 1. Temel KullanÄ±m

```bash
# Basit fine-tuning
python src/finetune.py \
    --dataset_path ./data/womens_clothing_reviews.csv \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --learning_rate 2e-4 \
    --bf16
```

### 2. Bellek Optimizasyonlu (KÃ¼Ã§Ã¼k GPU'lar iÃ§in)

```bash
# 8-12GB GPU iÃ§in
python src/finetune.py \
    --dataset_path ./data/womens_clothing_reviews.csv \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --max_seq_length 512 \
    --lora_r 8 \
    --lora_alpha 16 \
    --bf16
```

### 3. HÄ±zlÄ± Prototipleme

```bash
# HÄ±zlÄ± test iÃ§in
python src/finetune.py \
    --dataset_path ./data/womens_clothing_reviews.csv \
    --max_steps 200 \
    --per_device_train_batch_size 4 \
    --max_seq_length 512 \
    --learning_rate 5e-4
```

### 4. Production Kalitesi

```bash
# En iyi kalite iÃ§in
python src/finetune.py \
    --dataset_path ./data/womens_clothing_reviews.csv \
    --num_train_epochs 5 \
    --per_device_train_batch_size 8 \
    --lora_r 32 \
    --lora_alpha 64 \
    --learning_rate 1e-4 \
    --warmup_steps 100 \
    --report_to wandb \
    --bf16
```

## âš™ï¸ KonfigÃ¼rasyon

### Komut SatÄ±rÄ± ArgÃ¼manlarÄ±

| Parametre | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|------------|----------|
| `--model_id` | `meta-llama/Meta-Llama-3-8B-Instruct` | Temel model |
| `--dataset_path` | `./data/womens_clothing_reviews.csv` | Veri seti yolu |
| `--output_dir` | `./saved_models` | Ã‡Ä±ktÄ± dizini |
| `--num_train_epochs` | `3` | EÄŸitim epoch sayÄ±sÄ± |
| `--per_device_train_batch_size` | `4` | Batch size |
| `--gradient_accumulation_steps` | `4` | Gradient accumulation |
| `--learning_rate` | `2e-4` | Ã–ÄŸrenme oranÄ± |
| `--max_seq_length` | `1024` | Maksimum sequence uzunluÄŸu |
| `--lora_r` | `16` | LoRA rank boyutu |
| `--lora_alpha` | `32` | LoRA alpha parametresi |
| `--lora_dropout` | `0.05` | LoRA dropout oranÄ± |
| `--bf16` | `True` | BFloat16 precision |
| `--report_to` | `none` | Monitoring (`wandb`, `tensorboard`) |

### Ã–rnek KonfigÃ¼rasyonlar

`src/config_examples.py` dosyasÄ±nda farklÄ± kullanÄ±m senaryolarÄ± iÃ§in hazÄ±r konfigÃ¼rasyonlar bulabilirsiniz.

## ğŸ–¥ï¸ Sistem Gereksinimleri

### Minimum Gereksinimler

- **GPU**: 16GB+ VRAM (RTX 4090, A100, vb.)
- **RAM**: 32GB sistem belleÄŸi
- **Storage**: 50GB+ boÅŸ alan
- **CUDA**: 11.8 veya Ã¼zeri

### Ã–nerilen Gereksinimler

- **GPU**: 24GB+ VRAM (RTX 4090, A6000, A100)
- **RAM**: 64GB sistem belleÄŸi
- **Storage**: 100GB+ SSD
- **CUDA**: 12.0+

### KÃ¼Ã§Ã¼k GPU'lar iÃ§in

8-12GB GPU'lar iÃ§in memory optimized konfigÃ¼rasyon kullanÄ±n:
- Batch size: 1-2
- Gradient accumulation: 8-16
- Max sequence length: 512
- LoRA rank: 8

## ğŸ”§ Teknik Detaylar

### QLoRA OptimizasyonlarÄ±

- **4-bit Quantization**: NF4 formatÄ± ile %75 bellek tasarrufu
- **Double Quantization**: Ek bellek optimizasyonu
- **Paged AdamW**: Bellek verimli optimizer
- **Gradient Checkpointing**: Backward pass bellek optimizasyonu

### LoRA Parametreleri

- **Rank (r)**: 8-32 arasÄ± (kalite/hÄ±z dengesi)
- **Alpha**: Genellikle rank'Ä±n 2 katÄ±
- **Target Modules**: Attention ve MLP katmanlarÄ±
- **Dropout**: Overfitting'i Ã¶nlemek iÃ§in 0.05-0.1

### Dataset Ä°ÅŸleme

- **Instruction Formatting**: Llama-3 chat formatÄ±
- **Fashion-Specific Prompts**: Moda odaklÄ± talimat ÅŸablonlarÄ±
- **Sequence Packing**: Verimlilik iÃ§in batch optimizasyonu
- **Train/Test Split**: %90/%10 oranÄ±nda

## ğŸ“Š Monitoring ve Logging

### Weights & Biases

```bash
# W&B ile monitoring
pip install wandb
wandb login

python src/finetune.py \
    --report_to wandb \
    --run_name "aura-fashion-v1"
```

### Log DosyalarÄ±

- Training sÄ±rasÄ±nda otomatik log dosyasÄ± oluÅŸturulur
- Console'da rengli progress gÃ¶sterimi
- GPU memory kullanÄ±mÄ± tracking

## ğŸš¨ Troubleshooting

### Out of Memory HatasÄ±

```bash
# Batch size'Ä± azaltÄ±n
--per_device_train_batch_size 1
--gradient_accumulation_steps 16

# Sequence length'i azaltÄ±n
--max_seq_length 512

# LoRA rank'Ä± azaltÄ±n
--lora_r 8 --lora_alpha 16
```

### Model YÃ¼kleme HatasÄ±

```bash
# Hugging Face login kontrol
huggingface-cli whoami

# Cache temizle
rm -rf ~/.cache/huggingface/

# Model izin sayfasÄ±nÄ± ziyaret edin
# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```

### CUDA HatasÄ±

```bash
# CUDA sÃ¼rÃ¼mÃ¼nÃ¼ kontrol edin
nvidia-smi

# PyTorch CUDA uyumluluÄŸu
python -c "import torch; print(torch.cuda.is_available())"

# Uyumlu PyTorch sÃ¼rÃ¼mÃ¼ yÃ¼kleyin
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“ Ã‡Ä±ktÄ± DosyalarÄ±

EÄŸitim tamamlandÄ±ktan sonra `saved_models/` dizininde:

```
saved_models/
â””â”€â”€ llama3-8b-aura-fashion-qlora/
    â”œâ”€â”€ adapter_config.json       # LoRA konfigÃ¼rasyonu
    â”œâ”€â”€ adapter_model.safetensors # LoRA aÄŸÄ±rlÄ±klarÄ±
    â”œâ”€â”€ tokenizer.json           # Tokenizer dosyalarÄ±
    â”œâ”€â”€ tokenizer_config.json    
    â”œâ”€â”€ special_tokens_map.json  
    â”œâ”€â”€ finetune_config.json     # EÄŸitim konfigÃ¼rasyonu
    â”œâ”€â”€ README.md                # Model kartÄ±
    â””â”€â”€ logs/                    # Training loglarÄ±
```

## ğŸ”— Entegrasyon

EÄŸitilmiÅŸ model Aura conversational AI servisinde kullanÄ±m iÃ§in hazÄ±rdÄ±r:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Model yÃ¼kleme
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16
)

model = PeftModel.from_pretrained(
    base_model, 
    "./saved_models/llama3-8b-aura-fashion-qlora"
)

tokenizer = AutoTokenizer.from_pretrained(
    "./saved_models/llama3-8b-aura-fashion-qlora"
)
```

## ğŸ“ Destek

Herhangi bir sorun yaÅŸadÄ±ÄŸÄ±nÄ±zda:

1. Bu README'nin troubleshooting bÃ¶lÃ¼mÃ¼nÃ¼ kontrol edin
2. Log dosyalarÄ±nÄ± inceleyin
3. System requirements'larÄ± doÄŸrulayÄ±n
4. GitHub issues'da benzer problemleri arayÄ±n

---

**Aura AI Team** | 2025 | Version 1.0.0
